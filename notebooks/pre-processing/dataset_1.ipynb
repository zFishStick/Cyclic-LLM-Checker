{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99114df",
   "metadata": {},
   "source": [
    "## ðŸ”Ž Pre-processing of [Fake-News-Detection-dataset](https://huggingface.co/datasets/Pulk17/Fake-News-Detection-dataset)\n",
    "\n",
    "In this notebook, we will perform some operations in order to resize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e85dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_WORDS = \"Number of words\"\n",
    "NUM_ARTICLES = \"Number of Articles\"\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Pulk17/Fake-News-Detection-dataset/train.tsv\", sep=\"\\t\")\n",
    "\n",
    "df = df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "df = df.rename(columns={\"label\": \"is_fake_news\"})\n",
    "df[\"is_fake_news\"] = df[\"is_fake_news\"].map({0: True, 1: False})\n",
    "\n",
    "def df_text_length():\n",
    "    return df['text'].apply(\n",
    "        lambda x: (len(str(x)) - len(str(x).replace(' ', '')) + 1)\n",
    "        if isinstance(x, str) and str(x).strip() != '' else 0\n",
    "    )\n",
    "    \n",
    "    \n",
    "def create_histogram(ds, avg_length_article = 0.0):\n",
    "    plt.hist(ds, bins=100, color='blue')\n",
    "    plt.title(\"Distribution of News Article Lengths\")\n",
    "    plt.xlabel(NUM_WORDS)\n",
    "    plt.ylabel(NUM_ARTICLES)\n",
    "    plt.locator_params(axis='x', nbins=35)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.locator_params(axis='y', nbins=25)\n",
    "    if avg_length_article > 0.0:\n",
    "        plt.axvline(avg_length_article, color='red', linestyle='dashed', linewidth=1)\n",
    "        plt.text(avg_length_article + 50, 50, f'Average Length: {avg_length_article:.2f}', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f7d8b",
   "metadata": {},
   "source": [
    "### ðŸ§¾ Dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac16ea4",
   "metadata": {},
   "source": [
    "As we saw during the dataset exploration, inside of it there are 30k articles.\n",
    "<br>\n",
    "However, in order to evaluate the correctness of the outputs generated by LLMs, we want to resize and filter the dataset in order to produce a more compact and non redundant one.\n",
    "<br>\n",
    "First of all, we will show again the distribution of the length of articles (in terms of number of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df_text_length()\n",
    "\n",
    "most_length_article = df['text_length'].max()\n",
    "print(f\"Maximum article length (in words): {most_length_article}\")\n",
    "\n",
    "less_length_article = df['text_length'].min()\n",
    "print(f\"Minimum article length (in words): {less_length_article}\")\n",
    "\n",
    "avg_length_article = df['text_length'].mean()\n",
    "print(f\"Average article length (in words): {avg_length_article}\")\n",
    "\n",
    "create_histogram(df['text_length'], avg_length_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e3bb7",
   "metadata": {},
   "source": [
    "### ðŸ§¾ Deletion of duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c960ba7",
   "metadata": {},
   "source": [
    "The datasets contains duplicated entries. We will remove them to avoid bias in the training and evaluation of the models.\n",
    "We need to consider that the column ```id``` is unique for each row, so we will not consider it when looking for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c10e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=df.columns.difference(['id']))\n",
    "print(\"Size after removing duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11dbaf8",
   "metadata": {},
   "source": [
    "### ðŸ§¾ Dataset resizing\n",
    "In order to keep articles which are neither too short nor too long, we will make use of the standard deviation of the length of articles. This allow us to estabilish a range of acceptable lengths around the average length of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2389cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['text_length'].mean()\n",
    "std_dev = df['text_length'].std()\n",
    "\n",
    "print(\"Mean of article lengths:\", mean)\n",
    "print(\"Standard deviation of article lengths:\", std_dev)\n",
    "\n",
    "limit = 0.05 # indicates how many standard deviations from the mean we want to keep\n",
    "\n",
    "df_filtered = df[\n",
    "    (df['text_length'] >= mean - limit * std_dev) &\n",
    "    (df['text_length'] <= mean + limit * std_dev)\n",
    "]\n",
    "\n",
    "print(\"Size before filtering:\", df.shape[0])\n",
    "print(\"Remaining articles after filtering:\", df_filtered.shape[0])\n",
    "print(\"Article with min length after filtering:\", df_filtered['text_length'].min())\n",
    "print(\"Article with max length after filtering:\", df_filtered['text_length'].max())\n",
    "\n",
    "create_histogram(df_filtered['text_length'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3514e2",
   "metadata": {},
   "source": [
    "As it can be seen from th histogram above, we kept articles which length is in the range [400, 430] words. This will be our new dataset of reference for the evaluation of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77ab97",
   "metadata": {},
   "source": [
    "Here a graphical representation of the number of true vs fake news articles, to show if the dataset is still balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = df[\"is_fake_news\"].value_counts().plot(kind=\"bar\", color=[\"red\", \"green\"])\n",
    "plt.title(\"Distribution of Real vs Fake News\")\n",
    "plt.xlabel(\"Is Fake News\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width() / 2, # type: ignore\n",
    "        p.get_height() + 40, # type: ignore\n",
    "        f\"{int(p.get_height())}\", # type: ignore\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",   \n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"black\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "true_news = df[\"is_fake_news\"].value_counts(normalize=True) * 100\n",
    "fake_news = df[\"is_fake_news\"].value_counts(normalize=True) * 100\n",
    "print(f\"Percentage of Real News: {true_news[False]:.2f}%\")\n",
    "print(f\"Percentage of Fake News: {fake_news[True]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fe8f0",
   "metadata": {},
   "source": [
    "We can see that the dataset is quite balanced even after the filtering and resizing process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
